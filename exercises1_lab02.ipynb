{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChiaraSolito/ComputerVision/blob/main/exercises1_lab02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErRaPpt42Ucx"
      },
      "source": [
        "# Lab 02 - 1 #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRVLpqxd2Uc2"
      },
      "source": [
        "## Image classification on the Fashion-MNIST dataset using a ResNet-18 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXVeQju02Uc3"
      },
      "source": [
        "**1**\n",
        "\n",
        "* Create a custom `FMnistResNet18` class in which:\n",
        "    * Download the pre-trained ResNet-18\n",
        "    * Change the first and last layers\n",
        "\n",
        "Specifically, the input and output layers of a pre-trained ResNet-18 need to be changed, since ResNet was originally designed for ImageNet competition, which was a color (3-channel) image classification task with 1000 classes. Fashon-MNIST, on the other hand, only contains 10 classes, and itâ€™s images are in the grayscale (*i.e.*,1-channel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASr0d10r2Uc4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import ResNet18_Weights\n",
        "\n",
        "\n",
        "# Hyperparameters.\n",
        "LR = 3e-4\n",
        "EPOCH = 5\n",
        "BATCH_SIZE = 50\n",
        "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exeOXH7K2Uc6"
      },
      "outputs": [],
      "source": [
        "class FMnistResNet18(nn.Module):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(FMnistResNet18, self).__init__()\n",
        "\n",
        "        # Load the pre-trained ResNet-18 model from torchvision.models.\n",
        "        # TODO.\n",
        "\n",
        "        # Change the input layer to take grayscale images, instead of RGB images.\n",
        "        # TODO.\n",
        "\n",
        "        # Change the output layer to output 10 classes instead of 1000 classes.\n",
        "        # TODO.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Test the network, and verify the layers.\n",
        "test_my_resnet = FMnistResNet18()\n",
        "# [N, C, H, W]: batch N, channels C, depth D, height H, width W.\n",
        "dummy_input = torch.randn((32, 1, 244, 244))\n",
        "output = test_my_resnet(dummy_input)\n",
        "print(output.shape)\n",
        "\n",
        "# print(test_my_resnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J76LYaUF2Uc7"
      },
      "source": [
        "**2**\n",
        "\n",
        "* Create DataLoaders\n",
        "    * Hint: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "* Define the model\n",
        "* Define the loss function\n",
        "* Define the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tln-_2DJ2Uc7"
      },
      "outputs": [],
      "source": [
        "# Dataset.\n",
        "fashion_mnist = torchvision.datasets.FashionMNIST(download=True,\n",
        "                                                  train=True,\n",
        "                                                  root=\".\").train_data.float()\n",
        "\n",
        "# Transformations.\n",
        "data_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((fashion_mnist.mean()/255,), (fashion_mnist.std()/255,))])\n",
        "\n",
        "# DataLoaders.\n",
        "# TODO.\n",
        "\n",
        "# Define the model.\n",
        "# TODO.\n",
        "\n",
        "# Define the loss function.\n",
        "# TODO.\n",
        "\n",
        "# Define the optimizer.\n",
        "# TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuFIsmLm2Uc8"
      },
      "source": [
        "**3**\n",
        "\n",
        "* Write the training step\n",
        "    * Hint: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Blu06YVr2Uc9"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "\n",
        "# Training step.\n",
        "print(f\"Start training on {DEVICE} [...]\")\n",
        "\n",
        "for e in range(EPOCH):\n",
        "    e_loss = 0.0\n",
        "\n",
        "    for i, data in (tepoch := tqdm(enumerate(), unit=\"batch\", total=len())):\n",
        "        # TODO.\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1eLfg9-2Uc9"
      },
      "source": [
        "**4**\n",
        "\n",
        "* Write the evaluation step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc8X5W_B2Uc-"
      },
      "outputs": [],
      "source": [
        "# Evaluation step.\n",
        "t_loss = 0\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, data in (tepoch := tqdm(enumerate(), unit=\"batch\", total=len())):\n",
        "        # TODO.\n",
        "        pass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cv_and_dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}