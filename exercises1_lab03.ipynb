{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChiaraSolito/ComputerVision/blob/main/exercises1_lab03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuzQrMdb0lB2"
      },
      "source": [
        "# Lab 03 - 1 #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PDPXvbe0lB6"
      },
      "source": [
        "## Pedestrian Detection using Faster R-CNN ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5wwE9C00lB7"
      },
      "source": [
        "In this exercise we are going to explore a classic usage of a Faster RCNN model for pedestrian detection, using the COCO format.\n",
        "\n",
        "We are going to:\n",
        "* Download pycocotools to handle image datasets that use the COCO format\n",
        "* Define a dataset using the COCO format\n",
        "* Download a pre-trained Faster RCNN model\n",
        "* Edit the model to fine-tune it on our dataset\n",
        "* Train the model on our dataset\n",
        "\n",
        "In order to download the Penn-Fudan Database for Pedestrian Detection and Segmentation, run the following two commands:\n",
        "```\n",
        "!mkdir data\n",
        "!cd data && wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip && unzip -o PennFudanPed.zip &> /dev/null\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7g9ynLj0lB8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import cv2\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import contextlib\n",
        "import torchvision\n",
        "import torch.utils.data\n",
        "\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from PIL import Image\n",
        "# from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "from pycocotools.coco import COCO\n",
        "from matplotlib import pyplot as plt\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools import mask as coco_mask\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "\n",
        "# Seed.\n",
        "np.random.seed(66)\n",
        "torch.manual_seed(66)\n",
        "\n",
        "# Hyperparameters.\n",
        "EPOCH = 5\n",
        "BATCH_SIZE = 1\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz55lIvm0lB-"
      },
      "source": [
        "### Define the dataset ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdkjNig40lB-"
      },
      "outputs": [],
      "source": [
        "class PennFudanDataset(Dataset):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # Load all image files, sorting them to ensure that they are aligned.\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load images and masks.\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # Note that we haven't converted the mask to RGB, because each color\n",
        "        # corresponds to a different instance with 0 being background.\n",
        "        mask = Image.open(mask_path)\n",
        "        # Convert the PIL Image into a numpy array.\n",
        "        mask = np.array(mask)\n",
        "        # Instances are encoded as different colors.\n",
        "        obj_ids = np.unique(mask)\n",
        "        # The first id is the background, so remove it.\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # Split the color-encoded mask into a set of binary masks.\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # Get bounding box coordinates for each mask.\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # Convert everything into a torch.Tensor.\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # There is only one class.\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # Suppose all instances are not crowd.\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "        else:\n",
        "            img = np.asarray(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0jgJxDK0lB_"
      },
      "outputs": [],
      "source": [
        "def show_dataset(sample_data_loader):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plot_samples = 3\n",
        "    plot_cols = 3\n",
        "    _plot_idx = 0\n",
        "    for i, (image, metadata) in enumerate(sample_data_loader):\n",
        "        if i >= plot_samples:\n",
        "            break\n",
        "\n",
        "        _plot_idx += 1\n",
        "        plt.subplot(plot_samples, plot_cols, _plot_idx)\n",
        "        image = image.squeeze().numpy()\n",
        "        plt.imshow(image)\n",
        "\n",
        "        _plot_idx += 1\n",
        "        plt.subplot(plot_samples, plot_cols, _plot_idx)\n",
        "        boxes = metadata[\"boxes\"]  # 1, N, 4.\n",
        "        if len(boxes.shape) == 3:\n",
        "            boxes = boxes.squeeze(0)\n",
        "        image_with_bbox = image.copy()\n",
        "        for bbox in boxes:\n",
        "            xmin, ymin, xmax, ymax = bbox\n",
        "            xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
        "            pt1 = (xmin, ymin)\n",
        "            pt2 = (xmax, ymax)\n",
        "            cv2.rectangle(image_with_bbox, pt1, pt2, (0, 215, 0), 10)\n",
        "        plt.imshow(image_with_bbox)\n",
        "\n",
        "        _plot_idx += 1\n",
        "        # plt.subplot(plot_samples, plot_cols, _plot_idx)\n",
        "        # masks = metadata[\"masks\"].squeeze(0)\n",
        "        # s = len(masks.shape)\n",
        "        # if s == 3:\n",
        "        #     masks = torch.sum(masks, dim=0)\n",
        "\n",
        "        # m = masks.numpy().astype(np.uint8)\n",
        "        # m[m > 0] = 255\n",
        "        # plt.imshow(np.dstack((m, m, m)))\n",
        "\n",
        "# Perform data visualization.\n",
        "# TODO.\n",
        "\n",
        "# You may want to use a DataLoader, but it's not mandatory.\n",
        "# TODO.\n",
        "\n",
        "# For each sample show:\n",
        "# 1) The RGB image\n",
        "# 2) The Bounding Boxes over the RGB frame\n",
        "#    - HINT: use cv2.rectangle() to draw a rect, or matplotlib.patches.Rectangle()\n",
        "# TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogXe7hXj0lCA"
      },
      "source": [
        "### Modify the DNN ###\n",
        "\n",
        "We are going to change the classification head with one having the correct dimensions (features and classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzNYQvnD0lCB"
      },
      "outputs": [],
      "source": [
        "# Load a model pre-trained on COCO.\n",
        "# TODO.\n",
        "\n",
        "# Replace the classifier with a new one, that has num_classes which is user-defined.\n",
        "# 1 class (person) + background.\n",
        "# TODO.\n",
        "\n",
        "# Get number of input features for the classifier.\n",
        "# TODO.\n",
        "\n",
        "# Replace the pre-trained head with a new one.\n",
        "# TODO.\n",
        "\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMB1J3180lCB"
      },
      "outputs": [],
      "source": [
        "# Transformations.\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # If you want, many different Data Augmentation\n",
        "        # options are available for training phase.\n",
        "\n",
        "        # transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "        # RandomHorizontalFlip is one of them, but you may want to see also:\n",
        "        #   ColorJitter\n",
        "        #   RandomGrayscale\n",
        "        #   RandomAffine\n",
        "        pass\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "# Utils for train/test DataLoaders.\n",
        "def collate_fn(batch): \n",
        "    return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiOmhw1t0lCC"
      },
      "outputs": [],
      "source": [
        "# Dataset.\n",
        "# TODO.\n",
        "\n",
        "# Train and test split.\n",
        "# TODO.\n",
        "\n",
        "# DataLoaders.\n",
        "# TODO.\n",
        "\n",
        "# TODO.\n",
        "\n",
        "# Move model to the right device.\n",
        "# TODO.\n",
        "\n",
        "# Define the optimizer and a learning rate scheduler.\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params, \n",
        "    lr=0.005, \n",
        "    momentum=0.9, \n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3,\n",
        "                                               gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQR9lh040lCC"
      },
      "outputs": [],
      "source": [
        "# Training step.\n",
        "print(f\"Start training on {DEVICE} [...]\")\n",
        "\n",
        "def train_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    model.train()\n",
        "\n",
        "    for i, (images, targets) in (tepoch := tqdm(enumerate(data_loader), unit=\"batch\", total=len(data_loader))):\n",
        "        tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        # Step 1: send the image to the required device.\n",
        "        # Images is a list of B images (where B = batch_size of the DataLoader).\n",
        "        images = list(img.to(device) for img in images)\n",
        "\n",
        "        # Step 2: send each target to the required device\n",
        "        # Targets is a dictionary of metadata. each (k,v) pair is a metadata\n",
        "        # required for training.\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        model_time = time.time()\n",
        "        loss_dict = model(images, targets)\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        # Step 3. backward on loss.\n",
        "        # Normally, you would obtain the loss from the model.forward()\n",
        "        # and then just call .bacward() on it.\n",
        "        # In this case, for each task, you have a different loss, due to\n",
        "        # different error metrics adopted by the tasks.\n",
        "        # One typical approach is to combine all the losses to one single loss,\n",
        "        # and then then backward that single loss.\n",
        "        # In this way you can adjust the weight of the different tasks,\n",
        "        # multiplying each loss for a hyperparemeter.\n",
        "        # E.G.:\n",
        "        #       final_loss = loss_1 + gamma*(alpha*loss_2 + beta*loss_3)\n",
        "        # In this case, we want to sum up all the losses.\n",
        "        # TODO.\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    # TODO.\n",
        "    \n",
        "    # Update the learning rate.\n",
        "    lr_scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyIwKFWe0lCD"
      },
      "outputs": [],
      "source": [
        "class CocoEvaluator:\n",
        "    def __init__(self, coco_gt, iou_types):\n",
        "        assert isinstance(iou_types, (list, tuple))\n",
        "        coco_gt = copy.deepcopy(coco_gt)\n",
        "        self.coco_gt = coco_gt\n",
        "\n",
        "        self.iou_types = iou_types\n",
        "        self.coco_eval = {}\n",
        "        for iou_type in iou_types:\n",
        "            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n",
        "\n",
        "        self.img_ids = []\n",
        "        self.eval_imgs = {k: [] for k in iou_types}\n",
        "\n",
        "    def update(self, predictions):\n",
        "        img_ids = list(np.unique(list(predictions.keys())))\n",
        "        self.img_ids.extend(img_ids)\n",
        "\n",
        "        for iou_type in self.iou_types:\n",
        "            results = self.prepare(predictions, iou_type)\n",
        "            with contextlib.redirect_stdout(io.StringIO()):\n",
        "                coco_dt = COCO.loadRes(self.coco_gt, results) if results else COCO()\n",
        "            coco_eval = self.coco_eval[iou_type]\n",
        "\n",
        "            coco_eval.cocoDt = coco_dt\n",
        "            coco_eval.params.imgIds = list(img_ids)\n",
        "            img_ids, eval_imgs = coco_evaluate(coco_eval)\n",
        "\n",
        "            self.eval_imgs[iou_type].append(eval_imgs)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for iou_type in self.iou_types:\n",
        "            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n",
        "            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n",
        "\n",
        "    def accumulate(self):\n",
        "        for coco_eval in self.coco_eval.values():\n",
        "            coco_eval.accumulate()\n",
        "\n",
        "    def summarize(self):\n",
        "        for iou_type, coco_eval in self.coco_eval.items():\n",
        "            print(f\"IoU metric: {iou_type}\")\n",
        "            coco_eval.summarize()\n",
        "\n",
        "    def prepare(self, predictions, iou_type):\n",
        "        if iou_type == \"bbox\":\n",
        "            return self.prepare_for_coco_detection(predictions)\n",
        "        if iou_type == \"segm\":\n",
        "            return self.prepare_for_coco_segmentation(predictions)\n",
        "        if iou_type == \"keypoints\":\n",
        "            return self.prepare_for_coco_keypoint(predictions)\n",
        "        raise ValueError(f\"Unknown iou type {iou_type}\")\n",
        "\n",
        "    def prepare_for_coco_detection(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"bbox\": box,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, box in enumerate(boxes)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_segmentation(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            scores = prediction[\"scores\"]\n",
        "            labels = prediction[\"labels\"]\n",
        "            masks = prediction[\"masks\"]\n",
        "\n",
        "            masks = masks > 0.5\n",
        "\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            rles = [\n",
        "                mask_util.encode(np.array(mask[0, :, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0] for mask in masks\n",
        "            ]\n",
        "            for rle in rles:\n",
        "                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"segmentation\": rle,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, rle in enumerate(rles)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_keypoint(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "            keypoints = prediction[\"keypoints\"]\n",
        "            keypoints = keypoints.flatten(start_dim=1).tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"keypoints\": keypoint,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, keypoint in enumerate(keypoints)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
        "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
        "\n",
        "\n",
        "def merge(img_ids, eval_imgs):\n",
        "    import torch.distributed as dist\n",
        "\n",
        "    def is_dist_avail_and_initialized():\n",
        "        if not dist.is_available():\n",
        "            return False\n",
        "        if not dist.is_initialized():\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def get_world_size():\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return 1\n",
        "        return dist.get_world_size()\n",
        "\n",
        "    def all_gather(data):\n",
        "        world_size = get_world_size()\n",
        "        if world_size == 1:\n",
        "            return [data]\n",
        "        data_list = [None] * world_size\n",
        "        dist.all_gather_object(data_list, data)\n",
        "        return data_list\n",
        "\n",
        "    all_img_ids = all_gather(img_ids)\n",
        "    all_eval_imgs = all_gather(eval_imgs)\n",
        "\n",
        "    merged_img_ids = []\n",
        "    for p in all_img_ids:\n",
        "        merged_img_ids.extend(p)\n",
        "\n",
        "    merged_eval_imgs = []\n",
        "    for p in all_eval_imgs:\n",
        "        merged_eval_imgs.append(p)\n",
        "\n",
        "    merged_img_ids = np.array(merged_img_ids)\n",
        "    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n",
        "\n",
        "    # Keep only unique (and in sorted order) images.\n",
        "    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n",
        "    merged_eval_imgs = merged_eval_imgs[..., idx]\n",
        "\n",
        "    return merged_img_ids, merged_eval_imgs\n",
        "\n",
        "\n",
        "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n",
        "    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n",
        "    img_ids = list(img_ids)\n",
        "    eval_imgs = list(eval_imgs.flatten())\n",
        "\n",
        "    coco_eval.evalImgs = eval_imgs\n",
        "    coco_eval.params.imgIds = img_ids\n",
        "    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n",
        "\n",
        "\n",
        "def coco_evaluate(imgs):\n",
        "    with contextlib.redirect_stdout(io.StringIO()):\n",
        "        imgs.evaluate()\n",
        "    return imgs.params.imgIds, np.asarray(imgs.evalImgs).reshape(-1, len(imgs.params.areaRng), len(imgs.params.imgIds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-22S_0l90lCE"
      },
      "outputs": [],
      "source": [
        "class FilterAndRemapCocoCategories:\n",
        "    def __init__(self, categories, remap=True):\n",
        "        self.categories = categories\n",
        "        self.remap = remap\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        anno = target[\"annotations\"]\n",
        "        anno = [obj for obj in anno if obj[\"category_id\"] in self.categories]\n",
        "        if not self.remap:\n",
        "            target[\"annotations\"] = anno\n",
        "            return image, target\n",
        "        anno = copy.deepcopy(anno)\n",
        "        for obj in anno:\n",
        "            obj[\"category_id\"] = self.categories.index(obj[\"category_id\"])\n",
        "        target[\"annotations\"] = anno\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def convert_coco_poly_to_mask(segmentations, height, width):\n",
        "    masks = []\n",
        "    for polygons in segmentations:\n",
        "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
        "        mask = coco_mask.decode(rles)\n",
        "        if len(mask.shape) < 3:\n",
        "            mask = mask[..., None]\n",
        "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "        mask = mask.any(dim=2)\n",
        "        masks.append(mask)\n",
        "    if masks:\n",
        "        masks = torch.stack(masks, dim=0)\n",
        "    else:\n",
        "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
        "    return masks\n",
        "\n",
        "\n",
        "class ConvertCocoPolysToMask:\n",
        "    def __call__(self, image, target):\n",
        "        w, h = image.size\n",
        "\n",
        "        image_id = target[\"image_id\"]\n",
        "        image_id = torch.tensor([image_id])\n",
        "\n",
        "        anno = target[\"annotations\"]\n",
        "\n",
        "        anno = [obj for obj in anno if obj[\"iscrowd\"] == 0]\n",
        "\n",
        "        boxes = [obj[\"bbox\"] for obj in anno]\n",
        "        # guard against no boxes via resizing\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "        boxes[:, 2:] += boxes[:, :2]\n",
        "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "\n",
        "        classes = [obj[\"category_id\"] for obj in anno]\n",
        "        classes = torch.tensor(classes, dtype=torch.int64)\n",
        "\n",
        "        segmentations = [obj[\"segmentation\"] for obj in anno]\n",
        "        masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
        "\n",
        "        keypoints = None\n",
        "        if anno and \"keypoints\" in anno[0]:\n",
        "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
        "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
        "            num_keypoints = keypoints.shape[0]\n",
        "            if num_keypoints:\n",
        "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
        "\n",
        "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "        boxes = boxes[keep]\n",
        "        classes = classes[keep]\n",
        "        masks = masks[keep]\n",
        "        if keypoints is not None:\n",
        "            keypoints = keypoints[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = classes\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        if keypoints is not None:\n",
        "            target[\"keypoints\"] = keypoints\n",
        "\n",
        "        # for conversion to coco api\n",
        "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
        "        iscrowd = torch.tensor([obj[\"iscrowd\"] for obj in anno])\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def _coco_remove_images_without_annotations(dataset, cat_list=None):\n",
        "    def _has_only_empty_bbox(anno):\n",
        "        return all(any(o <= 1 for o in obj[\"bbox\"][2:]) for obj in anno)\n",
        "\n",
        "    def _count_visible_keypoints(anno):\n",
        "        return sum(sum(1 for v in ann[\"keypoints\"][2::3] if v > 0) for ann in anno)\n",
        "\n",
        "    min_keypoints_per_image = 10\n",
        "\n",
        "    def _has_valid_annotation(anno):\n",
        "        # If it's empty, there is no annotation.\n",
        "        if len(anno) == 0:\n",
        "            return False\n",
        "        # If all boxes have close to zero area, there is no annotation.\n",
        "        if _has_only_empty_bbox(anno):\n",
        "            return False\n",
        "        # Keypoints task have a slight different critera for considering\n",
        "        # if an annotation is valid.\n",
        "        if \"keypoints\" not in anno[0]:\n",
        "            return True\n",
        "        # For keypoint detection tasks, only consider valid images those\n",
        "        # containing at least min_keypoints_per_image.\n",
        "        if _count_visible_keypoints(anno) >= min_keypoints_per_image:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    assert isinstance(dataset, torchvision.datasets.CocoDetection)\n",
        "    ids = []\n",
        "    for ds_idx, img_id in enumerate(dataset.ids):\n",
        "        ann_ids = dataset.coco.getAnnIds(imgIds=img_id, iscrowd=None)\n",
        "        anno = dataset.coco.loadAnns(ann_ids)\n",
        "        if cat_list:\n",
        "            anno = [obj for obj in anno if obj[\"category_id\"] in cat_list]\n",
        "        if _has_valid_annotation(anno):\n",
        "            ids.append(ds_idx)\n",
        "\n",
        "    dataset = torch.utils.data.Subset(dataset, ids)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def convert_to_coco_api(ds):\n",
        "    coco_ds = COCO()\n",
        "    # Annotation IDs need to start at 1, not 0, see torchvision issue #1530.\n",
        "    ann_id = 1\n",
        "    dataset = {\"images\": [], \"categories\": [], \"annotations\": []}\n",
        "    categories = set()\n",
        "    for img_idx in range(len(ds)):\n",
        "        # Find better way to get target.\n",
        "        # targets = ds.get_annotations(img_idx)\n",
        "        img, targets = ds[img_idx]\n",
        "        image_id = targets[\"image_id\"].item()\n",
        "        img_dict = {}\n",
        "        img_dict[\"id\"] = image_id\n",
        "        img_dict[\"height\"] = img.shape[-2]\n",
        "        img_dict[\"width\"] = img.shape[-1]\n",
        "        dataset[\"images\"].append(img_dict)\n",
        "        bboxes = targets[\"boxes\"]\n",
        "        bboxes[:, 2:] -= bboxes[:, :2]\n",
        "        bboxes = bboxes.tolist()\n",
        "        labels = targets[\"labels\"].tolist()\n",
        "        areas = targets[\"area\"].tolist()\n",
        "        iscrowd = targets[\"iscrowd\"].tolist()\n",
        "        if \"masks\" in targets:\n",
        "            masks = targets[\"masks\"]\n",
        "            # Make masks Fortran contiguous for coco_mask.\n",
        "            masks = masks.permute(0, 2, 1).contiguous().permute(0, 2, 1)\n",
        "        if \"keypoints\" in targets:\n",
        "            keypoints = targets[\"keypoints\"]\n",
        "            keypoints = keypoints.reshape(keypoints.shape[0], -1).tolist()\n",
        "        num_objs = len(bboxes)\n",
        "        for i in range(num_objs):\n",
        "            ann = {}\n",
        "            ann[\"image_id\"] = image_id\n",
        "            ann[\"bbox\"] = bboxes[i]\n",
        "            ann[\"category_id\"] = labels[i]\n",
        "            categories.add(labels[i])\n",
        "            ann[\"area\"] = areas[i]\n",
        "            ann[\"iscrowd\"] = iscrowd[i]\n",
        "            ann[\"id\"] = ann_id\n",
        "            if \"masks\" in targets:\n",
        "                ann[\"segmentation\"] = coco_mask.encode(masks[i].numpy())\n",
        "            if \"keypoints\" in targets:\n",
        "                ann[\"keypoints\"] = keypoints[i]\n",
        "                ann[\"num_keypoints\"] = sum(k != 0 for k in keypoints[i][2::3])\n",
        "            dataset[\"annotations\"].append(ann)\n",
        "            ann_id += 1\n",
        "    dataset[\"categories\"] = [{\"id\": i} for i in sorted(categories)]\n",
        "    coco_ds.dataset = dataset\n",
        "    coco_ds.createIndex()\n",
        "    return coco_ds\n",
        "\n",
        "\n",
        "def get_coco_api_from_dataset(dataset):\n",
        "    for _ in range(10):\n",
        "        if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "            break\n",
        "        if isinstance(dataset, torch.utils.data.Subset):\n",
        "            dataset = dataset.dataset\n",
        "    if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "        return dataset.coco\n",
        "    return convert_to_coco_api(dataset)\n",
        "\n",
        "\n",
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transforms):\n",
        "        super().__init__(img_folder, ann_file)\n",
        "        self._transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super().__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        target = dict(image_id=image_id, annotations=target)\n",
        "        if self._transforms is not None:\n",
        "            img, target = self._transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def get_coco(root, image_set, transforms, mode=\"instances\"):\n",
        "    anno_file_template = \"{}_{}2017.json\"\n",
        "    PATHS = {\n",
        "        \"train\": (\"train2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"train\"))),\n",
        "        \"val\": (\"val2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"val\"))),\n",
        "        # \"train\": (\"val2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"val\")))\n",
        "    }\n",
        "\n",
        "    t = [ConvertCocoPolysToMask()]\n",
        "\n",
        "    if transforms is not None:\n",
        "        t.append(transforms)\n",
        "    transforms = T.Compose(t)\n",
        "\n",
        "    img_folder, ann_file = PATHS[image_set]\n",
        "    img_folder = os.path.join(root, img_folder)\n",
        "    ann_file = os.path.join(root, ann_file)\n",
        "\n",
        "    dataset = CocoDetection(img_folder, ann_file, transforms=transforms)\n",
        "\n",
        "    if image_set == \"train\":\n",
        "        dataset = _coco_remove_images_without_annotations(dataset)\n",
        "\n",
        "    # dataset = torch.utils.data.Subset(dataset, [i for i in range(500)])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_coco_kp(root, image_set, transforms):\n",
        "    return get_coco(root, image_set, transforms, mode=\"person_keypoints\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PknvGVKe0lCF"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    torch.set_num_threads(1)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
        "    coco_evaluator = CocoEvaluator(coco, [\"bbox\"])\n",
        "    model.eval()\n",
        "\n",
        "    for images, targets in data_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        model_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "\n",
        "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
        "        evaluator_time = time.time()\n",
        "        coco_evaluator.update(res)\n",
        "        evaluator_time = time.time() - evaluator_time\n",
        "\n",
        "    coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # Accumulate predictions from all images.\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()\n",
        "    torch.set_num_threads(n_threads)\n",
        "    \n",
        "    return coco_evaluator\n",
        "\n",
        "# Evaluation step.\n",
        "# TODO."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cv_and_dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}